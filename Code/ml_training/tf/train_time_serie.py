"""""
 *  \brief     train_time_serie.py
 *  \author    Jonathan Reymond
 *  \version   1.0
 *  \date      2023-02-14
 *  \pre       None
 *  \copyright (c) 2022 CSEM
 *
 *   CSEM S.A.
 *   Jaquet-Droz 1
 *   CH-2000 Neuch√¢tel
 *   http://www.csem.ch
 *
 *
 *   THIS PROGRAM IS CONFIDENTIAL AND CANNOT BE DISTRIBUTED
 *   WITHOUT THE CSEM PRIOR WRITTEN AGREEMENT.
 *
 *   CSEM is the owner of this source code and is authorised to use, to modify
 *   and to keep confidential all new modifications of this code.
 *
 """

import sys
import os

os.system("module load cuda/11.2")
os.system("module load cudnn/8.1")

import setGPU
# import tensorflow_models as tfm

from utils import *

from models import *

from utils import PrintCallback
sys.path.insert(0, 'ml_training/')
sys.path.insert(0, 'ml_training/preprocess_data')
from preprocess_data.prepare_data import get_rain_dataset
import dataloader

import numpy as np
import pandas as pd
from collections import Counter

import tensorflow as tf
import tensorflow_model_optimization as tfmot
from keras.callbacks import ReduceLROnPlateau


from sklearn.utils import class_weight

from functools import partial
import keras.backend as K


from itertools import combinations
import optuna
from optuna.trial import TrialState
import warnings
import joblib



smote_resampling = False
class_reweighting = True


batch_size = 16 * 2 * 2
test_size = 0.15
validation_size = 0.15
seed = 24
epochs = 35

def get_class_weight(labels):
    '''Returns the weights of each class corresponding to the number of times it appears

    Args:
        labels (1D array): labels array where classes consists of an integer range beginning from zero

    Returns:
        dict: dictionnary with keys equals to the integer range, and the values the corresponding weights
    '''
    y_integers = np.argmax(labels, axis=1)
    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_integers), y=y_integers)
    return dict(enumerate(class_weights))
   

    
def get_losses_and_metrics(labels_train, class_reweightings):
    '''Returns the losses and the metrics for each output, support multi-output. 
    The loss of each output is a (weighted) categorical cross-entropy and the metric the categorical accuracy

    Args:
        labels_train (dict): dict with keys the name of the label(wind and/or rain), and the values the labels themselves as 1D array
        class_reweightings (dict): dict with keys the name of the label(wind and/or rain) and the values boolean to get a weighted cross-entropy

    Returns:
        (dict, dict): dict of losses and metrics where the keys are the name of the labels
    '''
    loss = dict()
    metrics = dict()
    for label_name, labels in labels_train.items():
        cce = K.categorical_crossentropy
        if class_reweightings[label_name]:
            weights = get_class_weight(labels)
            cce = partial(weighted_categorical_crossentropy, weights_table=to_lookup_table(weights))
        loss.update({label_name : cce})
        metrics.update({label_name : 'categorical_accuracy'})
        
    return loss, metrics
                
            
def load_pretrained_model(dataset, class_reweightings=dict(rain=True, wind=True), return_compile_param=False, quant_name = False, learning_rate=1e-03):
    '''Load a pretrained model

    Args:
        dataset (tuple): tuple generated by dataloader.prepare_dataset
        class_reweightings (dict, optional): dict with keys the name of the label(wind and/or rain) and the values boolean to get a weighted cross-entropy. Defaults to dict(rain=True, wind=True).
        return_compile_param (bool, optional): _description_. Defaults to False.
        quant_name (bool, optional): If load the pretrained model to be retrain for QAT. Defaults to False.
        learning_rate (float, optional): learning of the optimizer. Defaults to 1e-03.

    Returns:
        Keras model: pretrained model
    '''
    num_classes, num_splits, inputs, labels, audio_length, sensor_shape = dataset
    model = get_model(num_classes= num_classes,
                    type_inputs=TYPE_INPUTS,
                    name_audio_model=NAME_AUDIO_MODEL,
                    name_sensor_model=NAME_SENSOR_MODEL,
                    audio_length=audio_length, 
                    sensor_shape=sensor_shape)

    optimizer = tf.keras.optimizers.Adam(
                            learning_rate=learning_rate)
    if quant_name:
        class_reweightings=dict(quant_rain=True, quant_wind=True)
        
    loss, metrics = get_losses_and_metrics(labels[0]['train'], class_reweightings)
        
    model.compile(optimizer=optimizer,
                loss=loss,
                metrics=metrics)
    model.load_weights(CHECKPOINT_FILEPATH).expect_partial()
    if return_compile_param:
        return model, dict(optimizer=optimizer, loss=loss, metrics=metrics)
    else :
        return model
    
    
    

def main(timesteps=None, step_size=None, dataset=None, class_reweightings=dict(rain=True, wind=True), 
         output_file='test.txt'):
    '''Main function

    Args:
        timesteps (Int, optional): timesteps to be done for time series: how much samples from past for each input. Defaults to None.
        step_size (Int, optional): step size for time series : t, t - step_size, t - 2*step_size. Defaults to None.
        dataset (tuple, optional): _description_. Defaults to None.
        class_reweightings (dict, optional): dict with keys the name of the label(wind and/or rain) and the values boolean to get a weighted cross-entropy. Defaults to dict(rain=True, wind=True).
        output_file (str, optional): output_file destination to write the results of the training. Defaults to 'test.txt'.

    Returns:
        (float, float): balanced accuracy for original problem, balanced accuracy for 2-classes reduction
    '''
    if dataset:
        num_classes, num_splits, inputs, labels, audio_length, sensor_shape = dataset
    else : 
        expand_dims = True
        num_classes, num_splits, inputs, labels, audio_length, sensor_shape = dataloader.prepare_dataset(TYPE_LABELS, TYPE_INPUTS, SPLIT_FACTOR,
                                                                                              timesteps, step_size, test_size, validation_size,
                                                                                              BEST_RAIN_IDX, BEST_WIND_IDX, expand_dims)
    
    print('Model selected:', NAME_AUDIO_MODEL)

    # artefact
    num_split = 0
        

    model = get_model(num_classes= num_classes,
                    type_inputs=TYPE_INPUTS,
                    name_audio_model=NAME_AUDIO_MODEL,
                    name_sensor_model=NAME_SENSOR_MODEL,
                    audio_length=audio_length, 
                    sensor_shape=sensor_shape)

    optimizer = tf.keras.optimizers.Adam(
                            learning_rate=1e-03)
    loss, metrics = get_losses_and_metrics(labels[num_split]['train'], class_reweightings)
        
    model.compile(optimizer=optimizer,
                loss=loss,
                metrics=metrics)

    if num_split == 0:
        print(model.summary())
        print("--------------------------")
        # print("GFlops :", get_flops(model))
        # keras.utils.plot_model(model, "my_first_model_with_shape_info.png", show_shapes=True)
        check_memory(model, with_assertion=False)

    # Define callbacks
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=6e-08, verbose=1)
    

    c_matrix_callback = ConfusionMatrixCallback(inputs[num_split]['val'], labels[num_split]['val'], model.output_names)
    
    
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=CHECKPOINT_FILEPATH,
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True)

    callbacks = [reduce_lr, c_matrix_callback, model_checkpoint_callback]
    if USE_TENSORBOARD:
        tb_callback = tf.keras.callbacks.TensorBoard(TF_FOLDER + 'logs', histogram_freq = 1, update_freq='epoch')
        callbacks.append(tb_callback)


    model.fit(x=inputs[num_split]['train'],
            y=labels[num_split]['train'],
            batch_size=batch_size,
            epochs=epochs,
            verbose=2,
            shuffle=True,
            validation_data=(inputs[num_split]['val'], labels[num_split]['val']),
            use_multiprocessing = True
            , callbacks = callbacks)


    print("=============================")
    print('Finished training, evaluation')
    print("=============================")
    model.load_weights(CHECKPOINT_FILEPATH).expect_partial()
    print()
    outputs = model.predict(inputs[num_split]['test'], verbose=0)
    # outputs_dict = {name: pred for name, pred in zip(model.output_names, outputs)}
    if len(model.output_names) == 1:
        outputs_dict = {model.output_names[0] : outputs}
    else :
        outputs_dict = {name: pred for name, pred in zip(model.output_names, outputs)}
        
    for name, output in outputs_dict.items():
        y_pred = output_to_pred(output)
        acc_3, acc_2 = get_results(y_pred, labels[num_split]['test'], name, output_file)
        
    tf.saved_model.save(model, MODEL_FILEPATH)
    return acc_3, acc_2


   
def set_label(type_label):
    '''change global TYPE_LABEL during runtime

    Args:
        type_label (enum Label): new value of label
    '''
    global TYPE_LABELS
    TYPE_LABELS = type_label
 
 
def set_best_idx(best_rain_idx, best_wind_idx):
    '''change global splitting values for WIND and RAIN

    Args:
        best_rain_idx (list integers): new splitting values for rain sorted
        best_wind_idx (list integers): new splitting values for wind sorted
    '''
    global BEST_RAIN_IDX
    BEST_RAIN_IDX = best_rain_idx
    global BEST_WIND_IDX
    BEST_WIND_IDX = best_wind_idx
    


if __name__ == '__main__':
    sys.path.insert(1, 'ml_training/')
    tf.config.run_functions_eagerly(False)
    print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))

    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)

    output_file = '/local/user/jrn/tinyml-challenge-2022/ohllsdflsalfdlsadfldslfsa.txt'

    main(output_file=output_file)

        
    
########################################################################
##################  For Optuna training: need updates ##################
########################################################################
  
# def objective(trial):
#     timesteps = trial.suggest_categorical("timesteps", [60, 80, 100, 120, 150, 170, 200, 220, 250, 270, 300, 320, 350, 370, 400])
#     step_size = trial.suggest_categorical("step_size", [1, 2, 4, 6])
#     if (timesteps * step_size) > 400 or (timesteps * step_size) < 50:
#         print('too much/less')
#         return 0
#     print("Arguments:", timesteps, step_size)
#     dataset = prepare_dataset(TYPE_LABELS, TYPE_INPUTS, SPLIT_FACTOR, timesteps, step_size)
#     acc_3, acc_2 = main(dataset=dataset)
#     #ad-hoc if doesn't work ternary
#     return acc_3
      
    
# if __name__ == "__main__":
#     study = optuna.create_study(direction="maximize", 
#                                 study_name='sensor_optuna_3')
    
    
#     study.optimize(objective, n_trials=80, callbacks=[PrintCallback()])

#     print("Study statistics: ")
#     print("  Number of finished trials: ", len(study.trials))

#     print("Best trial:")
#     trial = study.best_trial

#     print("  Value: ", trial.value)

#     print("  Params: ")
#     for key, value in trial.params.items():
#         print("    {}: {}".format(key, value))
    
#     joblib.dump(study, "/local/user/jrn/tinyml-challenge-2022/results/sensor_study_3.pkl")
    
    
    