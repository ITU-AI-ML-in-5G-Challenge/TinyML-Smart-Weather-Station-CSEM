"""""
 *  \brief     extract_data.py
 *  \author    Jonathan Reymond
 *  \version   1.0
 *  \date      2023-02-14
 *  \pre       None
 *  \copyright (c) 2022 CSEM
 *
 *   CSEM S.A.
 *   Jaquet-Droz 1
 *   CH-2000 Neuch√¢tel
 *   http://www.csem.ch
 *
 *
 *   THIS PROGRAM IS CONFIDENTIAL AND CANNOT BE DISTRIBUTED
 *   WITHOUT THE CSEM PRIOR WRITTEN AGREEMENT.
 *
 *   CSEM is the owner of this source code and is authorised to use, to modify
 *   and to keep confidential all new modifications of this code.
 *
 """
 import numpy as np
import pandas as pd
import os
import wave
import pickle
import librosa
from constants import *


def extract_sensors_files(hour):
    '''extract files from one hour mic recording, transform .wav into numpy array

    Args:
        hour : hour of desired recording

    Returns:
        audio_data : numpy array of the audio recording
        df : dataframe of sensor with columns=['humidity', 'pressure', 'temperature', 'timestamp']
        
    '''

    path_wav = WAVE_FOLDER + get_filename(hour, '.wav')
    # want to load it into digital format = [-1, 1], float32
    audio_data, sr = librosa.load(path_wav, sr=None)
    assert sr == SAMPLING_FREQUENCY, 'different sampling frequency as SAMPLING_FREQUENCY, got ' + str(sr)
    
    path_pkl = PICKLE_FOLDER + get_filename(hour, '.pkl')
    sensor_df = pd.read_pickle(path_pkl)  

    return np.float32(audio_data), sensor_df




def create_sensor_df(hour):
    '''Extract and create microphone audio into a dataframe given filename, preprocess it and rescale the timestamps in the [sec] scale

    Args:
        hour (int): hour or recording desired

    Returns:
        dataframe: audio dataframe with as row: [timestamp, [value array of audio]], column names = ['timestamp', 'audio']
    '''
    audio, sensor_df = extract_sensors_files(hour)

    rest = len(audio) % SAMPLING_FREQUENCY
    if rest != 0:
        print('remove tail of audio, rest=', rest)
        audio = audio[: len(audio) -rest]
        #remove last entry
        sensor_df.drop(sensor_df.tail(1).index,inplace=True)


    # sort w.r.t timestamp
    sensor_df.sort_values('timestamp', inplace=True)
    #change from mms to sec
    # sensor_df['timestamp'] *= 1/1000
    #add start timestamp for absolute time
    # sensor_df['timestamp'] += MIC_START_TIMESTAMP
    #add samples into the dataframe
    audio_chunk = np.split(audio, int(len(audio) / SAMPLING_FREQUENCY))
    sensor_df["audio"] = audio_chunk
    return sensor_df.drop_duplicates(subset="timestamp")


#######################################################################################
################################### Weather station ###################################
#######################################################################################

def correct_log_file(station_filename):
    '''correct the log file generated by the spark openlog : if multiple times or no header

    Args:
        station_filename (str): filename of log generated by the spark openlog, of the form '<folder path>/LOGXXXX.TXT'
    '''
    header = 'wind_dir,wind_count,rain_count'
    station_filename_temp = station_filename.replace(".TXT", '_temp.TXT')

    with open(station_filename) as fin, open(station_filename_temp, "w+") as fout:
        for line in fin:
            if line is not header:
                fout.write(line)

    os.remove(station_filename)
    os.rename(station_filename_temp, station_filename)


def make_diff(array, index, treshold=True):
    '''auxiliary function to modify the count to get the count in each time slot and not the overall count

    Args:
        array (list[int]): list of overall counts
        index (int): index of interest
        treshold (bool, optional): _description_. Defaults to True.

    Returns:
        int: the count for the time slot defined by the index
    '''
    diff = array[index] - array[index - 1]
    #dued to threshold
    if diff < 0 :
        diff = THRESHOLD - diff
    return diff



def preprocess_weather_station(station_df, wind_vector):
    '''preprocess weather station data

    Args:
        station_df (dataframe): the raw station dataframe generated in create_station_df
        wind_vector (bool): if we want to add a vector for the wind direction

    Returns:
        dataframe: processed station dataframe
    '''

    # instead of counting the overall count : just count in the time slot, assume wind and rain not counted twice
    # do rain after to track every exception since more rare than wind interrupt + rain duration longer
    wind = station_df['wind_count'].to_list()
    wind_new = [0]
    for i in range(1, len(station_df)):
        wind_new.append(make_diff(wind, i))
 
    station_df['wind_count'] = wind_new.copy()

    #replace '???' values by previous valid direction encountered
    previous_dir = station_df['wind_dir'][station_df['wind_dir'] != '???'][0]
    wind_dir_new = []
    for wind_dir in station_df['wind_dir']:
        if wind_dir != '???':
            previous_dir = wind_dir
        wind_dir_new.append(previous_dir)
    station_df['wind_dir'] = wind_dir_new

    # transform direction to int
    station_df['wind_dir'] = station_df['wind_dir'].map(WIND_DIRECTIONS)
    if wind_vector:
        #create a vector of the wind with magnitude = wind_count
        station_df["wind_x"] = station_df["wind_count"] * np.cos(station_df["wind_dir"] * RAD_ANGLE)
        station_df["wind_y"] = station_df["wind_count"] * np.sin(station_df["wind_dir"] * RAD_ANGLE)
    
    return station_df


def get_linear_value(x, point1, point2):
    x1, y1 = point1
    x2, y2 = point2
    slope = (y2 - y1)/(x2 - x1)
    intercept = - slope * x1 + y1
    return slope * x + intercept


def correct_timestamp(station_df):
    # assume already sorted + in seconds
    time_pairs = pd.read_csv(TIMESTAMP_COMPARISON_FILE, header=0)[:-1]
    time_pairs = time_pairs[["timestamp microbit", "timestamp max"]]
    
    new_timestamps = []
    comp_idx = 0
    df_idx = 0
    # drop entries for which have not 2 points
    station_df = station_df[(time_pairs['timestamp microbit'].iloc[0] <= station_df['timestamp']) &
                             (station_df['timestamp'] < time_pairs['timestamp microbit'].iloc[-1])]
        
    for df_idx in range(len(station_df)):
        station_entry = station_df['timestamp'].iloc[df_idx]
        while time_pairs['timestamp microbit'].iloc[comp_idx] <= station_entry:
            comp_idx += 1
        if comp_idx > 0:
            output_val = get_linear_value(station_entry, tuple(time_pairs.iloc[comp_idx-1]), tuple(time_pairs.iloc[comp_idx]))
            new_timestamps.append(output_val)
        else :
            print('this should never happen')
            
    res_df = station_df.copy()
    res_df['timestamp'] = new_timestamps
    return res_df
    
    

def create_station_df(wind_vector=False):
    '''create the weather station dataframe from the files generated by the spark openlog

    Args:
        wind_vector (bool, optional): if we want to add a vector for the wind direction. Defaults to False.

    Returns:
        dataframe: weather station dataframe preprocessed
    '''
    station_filename = STATION_FILENAMES()[0]
    correct_log_file(station_filename)
    station_df = pd.read_csv(station_filename, header=0, names=STATION_HEADER)
    station_df.sort_values('timestamp', inplace=True)

    #change from mms to sec
    # station_df['timestamp'] *= (1/1000)
    #add start timestamp for correct time
    # station_df['timestamp'] += STATION_START_TIMESTAMP
    station_df = preprocess_weather_station(station_df, wind_vector)
    station_df = correct_timestamp(station_df)
    return station_df.sort_values('timestamp')

###############################################################################
################################### General ###################################
###############################################################################

def find_nearest_timestamp(sensor_time, sensor_idx, station_time, station_idx=0):
    '''for a given sensor timestamp, find corresponding weather station timestamp

    Args:
        sensor_time (list[int]): sensor timestamp list
        sensor_idx (int): index of sensor timestamp we want to infer the corresponding timestamp
        station_time (list[int]): station timestamp list
        station_idx (int, optional): starting index of the weather station timestamp to consider. Defaults to 0.

    Returns:
        int: the weather station timestamp index for the given sensor timestamp
    '''
    s_t = sensor_time[sensor_idx]
    #ensure the station recording is after the timestamp
    while station_time[station_idx] <= s_t:
        station_idx += 1

    #ensure to have the latest recording, i.e. just before the next sensor recording
    # TODO : think if we want to add the previous results if one station row skipped
    # TODO : think handle case no weather station recording between 

    return station_idx


# assume station time and sensor time sorted, sort station time
def merge_dataframes(sensor_df, station_df):
    '''merge the sensor dataframe and the weather station dataframe

    Args:
        sensor_df (dataframe): sensor dataframe
        station_df (dataframe): weather station dataframe

    Returns:
        dataframe: resulting merged dataframe
    '''

    # mask_time = (REAL_START_TIMESTAMP <= sensor_df['timestamp']) & (sensor_df['timestamp'] <= REAL_END_TIMESTAMP)
    mask_time = (station_df['timestamp'].iloc[0] <= sensor_df['timestamp']) & \
                (sensor_df['timestamp'] <= station_df['timestamp'].iloc[-1])
                
    sensor_time = sensor_df[mask_time]['timestamp'].to_list()
    print("length of sensor_time:", len(sensor_time))

    station_time = station_df['timestamp'].to_list()
    station_idx = 0

    merged_list = []
        
    for sensor_idx in range(len(sensor_time) - 1):
        station_idx = find_nearest_timestamp(sensor_time, sensor_idx, station_time, station_idx)
        row = {'timestamp' : station_time[station_idx]}
        row.update({'timestamp_max' : sensor_time[sensor_idx]})

        row.update(sensor_df.drop(columns='timestamp').iloc[sensor_idx].to_dict())
        row.update(station_df.drop(columns='timestamp').iloc[station_idx].to_dict())

        merged_list.append(row)

    merged_df = pd.DataFrame.from_records(merged_list)

    # transform to interrupts rather than continuous measure
    rain = merged_df['rain_count'].to_list()
    rain_new = [0]
    for i in range(1, len(merged_df)):
        rain_new.append(make_diff(rain, i))
    merged_df['rain_count'] = rain_new.copy()

    merged_filtered_df = filter_merged_df(merged_df)
    return merged_filtered_df


def filter_merged_df(merged_df):
    # BME sensor values check [pressure from possible range in atmospheric pressure]
    mask_humidity = (merged_df['humidity'] <= 100) & (merged_df['humidity'] >= 0)
    print("number of invalid humidity measure(s) :", len(merged_df) - len(merged_df[mask_humidity]))
    mask_temperature = (merged_df['temperature'] <= 85) & (merged_df['temperature'] >= -40)
    print("number of invalid temperature measure(s) :", len(merged_df) - len(merged_df[mask_temperature]))
    mask_pressure = (merged_df['pressure'] <= 1100 * 100) & (merged_df['pressure'] >= 870 * 100)
    print("number of invalid pressure measure(s) :", len(merged_df) - len(merged_df[mask_pressure]))

    mask = mask_humidity & mask_temperature & mask_pressure
    print("total of recordings dropped:", len(merged_df) - len(merged_df[mask]))
    filtered_df = merged_df[mask]

    # Crop the dataframe to remove start and end recording
    mask_time = (filtered_df['timestamp'] <= REAL_END_TIMESTAMP) & (filtered_df['timestamp'] >= REAL_START_TIMESTAMP)
    print("Cropped recording : initial size =", len(filtered_df), ",output size =", len(filtered_df[mask_time]))
    return filtered_df[mask_time]
        


if __name__ == '__main__':
    print("=======================")
    print("Extracting station dataframe...")
    station_df = create_station_df(WIND_VECTOR)
    
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)

    for hour in INDEX_HOURS:
        print("Extracting sensor dataframe, hour :", hour)
        sensor_df = create_sensor_df(hour)
        print("merging the two dataframes ...")
        merged_df = merge_dataframes(sensor_df, station_df)
        print("storing the merged dataframe...")
        merged_df.to_pickle(get_result_filename(hour))
        print('---------------------------------------')


    print('Extraction done')






